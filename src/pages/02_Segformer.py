# src/pages/02_Segformer.py
import streamlit as st

st.set_page_config(
    page_title="2. SegFormer",
    page_icon="游댌",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        "About": "Esta app fue creada por Marce para demo de Segmentaci칩n Sem치ntica."
    }
)

st.markdown("""
<style>
    .main {
        padding: 2rem 4rem;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        background: #f5f7fa;
        color: #222222;
    }
    h1, h2, h3 {
        color: #0d6efd;
    }
    table {
        border-collapse: collapse;
        width: 50%;
        margin-top: 1rem;
        margin-bottom: 2rem;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px 12px;
        text-align: left;
    }
    th {
        background-color: #0d6efd;
        color: white;
    }
    a {
        color: #0d6efd;
        font-weight: bold;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

st.title("2. Arquitectura detallada de SegFormer")

st.markdown("""
## Introducci칩n

SegFormer propone un dise침o simple pero eficiente para segmentaci칩n sem치ntica basado en Transformers, que combina un encoder jer치rquico con un decoder ligero y sin uso de positional embeddings, permitiendo alta precisi칩n y velocidad.
""")

st.image("data/images/003_segformer_img.png", caption="Segmentaci칩n Sem치ntica", width=900)
st.image("data/images/000_Architecture.png", caption="Arquitectura SegFormer (Encoder + Decoder)", width=900)

st.markdown("""
## Encoder: Mix Transformer (MiT)

- El encoder es un Transformer jer치rquico con m칰ltiples etapas, inspirado en arquitecturas de visi칩n como Swin Transformer pero con simplificaciones.
- Cada etapa procesa la imagen a distintas resoluciones, extrayendo caracter칤sticas con diferentes escalas espaciales (1/4, 1/8, 1/16, 1/32 del tama침o original).
- El encoder emplea **mezcla local y global** mediante la auto-atenci칩n eficiente con ventanas y reducci칩n progresiva, logrando un buen balance entre precisi칩n y eficiencia.
- MiT-B0, la versi칩n m치s ligera usada en este modelo, tiene **3.7 millones de par치metros** en el encoder.

## Decoder: MLP ligero y eficiente

- En lugar de decoders complejos o basados en convoluciones, SegFormer utiliza un **decoder simple basado en multilayer perceptrons (MLP)**.
- El decoder toma las caracter칤sticas jer치rquicas extra칤das en diferentes escalas, las proyecta a un espacio com칰n y las combina para producir las predicciones finales de segmentaci칩n.
- Esta simplicidad reduce par치metros y costos computacionales, permitiendo inferencia r치pida.
- El decoder tiene alrededor de **0.4 millones de par치metros**.

## Ventajas clave

- **Sin positional embeddings:** SegFormer elimina la necesidad de embeddings posicionales expl칤citos, usando mecanismos de atenci칩n y pooling que mantienen la informaci칩n espacial y permiten inferencia flexible en im치genes de diferentes tama침os.
- **Jerarqu칤a eficiente:** La arquitectura jer치rquica captura tanto detalles finos como contexto global.
- **Alta velocidad y precisi칩n:** En resoluci칩n 512x512, SegFormer puede alcanzar hasta **47 FPS** en GPU, con resultados de precisi칩n competitivos en datasets est치ndares como ADE20k y Cityscapes.
- **Dise침o modular:** Facilita adaptaci칩n a diferentes tama침os de modelo (MiT-B0 a MiT-B5) y a distintas tareas de segmentaci칩n.

## Par치metros y desempe침o (MiT-B0)

<table>
    <tr><th>Componente</th><th>Par치metros (millones)</th></tr>
    <tr><td>Encoder</td><td>3.7</td></tr>
    <tr><td>Decoder</td><td>0.4</td></tr>
    <tr><th>Total</th><th>~4.1</th></tr>
</table>

## Referencias

- Xie et al., "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers", *NeurIPS 2021*.
- [Paper completo](https://arxiv.org/abs/2105.15203)
""", unsafe_allow_html=True)
